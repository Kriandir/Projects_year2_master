{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f79a70a1-842d-5acd-d706-01fc7584fc0b"
   },
   "source": [
    "# Final Project:  Spam filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "You’re the project manager for an enterprise email system and assigned a task to develop a spam filter for a company's email system. We’ve collected email samples that have been validated to be spam or non-spam emails. Your task is to predicts whether an email contains spam or not.\n",
    "\n",
    "You are given 3068 training emails with two classes: \"spam\" or \"not spam\". Using these data, you are expected to build your own spam filter with the kownledge you learned from this course. The goal is to correctly classify 1292 test emails. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "- **Spam email**\n",
    "\n",
    "> Subject: younger and healthier with ultimate - hghl 7283  as seen on nbc , cbs , cnn , and even oprah ! the health discovery that actuallyreverses aging while burning fat , without dieting or exercise ! this provendiscovery has even been reported on by the new england journal of medicine . forget aging and dieting forever ! and it ' s guaranteed !  click below to enter our web site :  http : / / www . freehostchina . com / washgh /  would you like to lose weight while you sleep !  no dieting !  no hunger pains !  no cravings !  no strenuous exercise !  change your life forever !  100 % guaranteed !  1 . body fat loss 82 % improvement .  2 . wrinkle reduction 61 % improvement .  3 . energy level 84 % improvement .  4 . muscle strength 88 % improvement .  5 . sexual potency 75 % improvement .  6 . emotional stability 67 % improvement .  7 . memory 62 % improvement .  click below to enter our web site :  http : / / www . freehostchina . com / washgh /  if you want to get removed  from our list please email at - standardoptout @ x 263 . net ( subject = remove \" your email \" )\n",
    "\n",
    "- **Not-spam email**\n",
    "\n",
    "> Subject: december 6 th meeting  dear mr . kaminski :  this is to confirm the december 6 th meeting here at our center .  the location for the meeting is room # 3212 steinberg hall - dietrich hall and  the time will run from 9 : 00 am - 11 : 00 am .  please let us know if you need anything further .  we look forward to seeing you then .  regards ,  theresa convery  ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~  theresa convery  administrative assistant  risk and decision processes center  the wharton school of the university of pennsylvania  ( 215 ) 898 - 5688 / fax : ( 215 ) 573 - 2130  tconvery @ wharton . upenn . edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of solution\n",
    "\n",
    "\n",
    "**1. Text representation.**\n",
    "\n",
    "As you can see, text content of emails is unstructured data. To apply machine learning methods on top of them, we first need to extract structured feature. To demonstrate this, we'll show using bag-of-word model for textural represention.\n",
    "\n",
    "** 2. Build your classifier. **\n",
    "\n",
    "As baseline, we provide solution based on *SVM* (Support Vector Machine).\n",
    "\n",
    "** 3. Evaluation **\n",
    "\n",
    "We will use *AP* (Average Precision) and *Accuracy* for performance evaluation in this notebook.\n",
    "\n",
    "Note that for evaluation on kaggle competition, [*MCE*](https://www.kaggle.com/wiki/MeanConsequentialError) (Mean Consequential Error) is used instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python package dependence\n",
    "- **pandas**   : for loading CSV files;\n",
    "- **nltk**     ：for word pre-processing;\n",
    "- **wordcloud**: for data visulization.\n",
    "\n",
    "Tips: To install missing packages, you can either do \"pip install package_name\" or \"conda install package_name\" in case of anaconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e2c1ada8-d68d-4e8d-e807-6e47ea7f5a58"
   },
   "source": [
    "# Data preparation\n",
    "\n",
    "## 1) Download data.\n",
    "\n",
    "Download \"emails.train.csv\", \"emails.test.csv\" from our kaggle competition page [here](https://www.kaggle.com/c/spamfilter-aml-uva/data), and put it under the same folder as this ipython notebook.\n",
    "\n",
    "\n",
    "\n",
    "## 2) Read in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e14bfbc8-92b8-4b60-081d-1f6fc0728ab1",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "assert os.path.exists('./emails.train.csv'), \"[Dataset File Not Found] Please download dataset first.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "204b6823-c7ea-8f47-a62b-f649f82c71ee",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in csv file as dataframe\n",
    "df = pd.read_csv('./emails.train.csv')\n",
    "zf = pd.read_csv('./emails.test.csv')\n",
    "# Show a snippet of dataset.\n",
    "print df.head()\n",
    "print np.sum(zf['spam'])\n",
    "spamappend = 0\n",
    "hamappend = 0\n",
    "for i in df[\"spam\"]:\n",
    "    if i == 1:\n",
    "        spamappend += 1\n",
    "    if i == 0:\n",
    "        hamappend +=1\n",
    "\n",
    "probspam = float(spamappend)/(spamappend+hamappend)\n",
    "print probspam\n",
    "spam = sum(df[\"spam\"])\n",
    "ham =  len(df[\"spam\"])-sum(df[\"spam\"])\n",
    "probspam = float(spam)/(spam+ham)\n",
    "print probspam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, each emails has two fields: \n",
    "* \"text\": the full text content of an email.\n",
    "* \"spam\": an integer flag to mark whether an email is a spam (=1) or not (=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"===========================\")\n",
    "print(\"Example of spam emails\")\n",
    "print(\"---------------------------\")\n",
    "\n",
    "df_pos = df[df['spam']==1]\n",
    "\n",
    "print( np.random.choice(df_pos['text']) ) \n",
    "\n",
    "print(\"===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"===========================\")\n",
    "print(\"Example of not spam emails\")\n",
    "print(\"---------------------------\")\n",
    "\n",
    "df_neg = df[df['spam']==0]\n",
    "\n",
    "print( np.random.choice(df_neg['text']) ) \n",
    "\n",
    "print(\"===========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text representation\n",
    "\n",
    "## 1) Create the bags of words vocabulary. \n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. (from [wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIT ITEREERT DUS OVER DE PANDAS DATAFRAME\n",
    "def select_vocabulary(dataframe, topN=10000):\n",
    "    # for w in dataframe.str.lower():\n",
    "    word2freq = dict()\n",
    "    for line in dataframe:\n",
    "        words = line.split()\n",
    "#       met setdefault fixed ie de value in de dict op 0 tenzij er iets te tellen valt dan +1\n",
    "        for word in words:\n",
    "            word2freq[word] = word2freq.setdefault(word, 0) +1\n",
    "\n",
    "    word_freq = [ (word,freq) for word,freq in word2freq.items() ]\n",
    "    \n",
    "    # sort according to freq, in descending order.\n",
    "    word_freq.sort(key=lambda x:x[1], reverse=True) \n",
    "    \n",
    "    # show selection results\n",
    "    print(\"%-10s  %10s\" % ('word', 'frequency'))\n",
    "    print(\"-------------------------\")\n",
    "    for i in range(15):\n",
    "        print(\"%-10s  %10d\" % (word_freq[i]))\n",
    "    print(\"...\\n\")\n",
    "    \n",
    "#     returned top 100 meest voorkomende woorden\n",
    "    return([x[0] for x in word_freq[:topN]])\n",
    "\n",
    "vocabulary = select_vocabulary(df['text'])\n",
    "word2ind   = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "# dus als je print vocabulary[0] krijg je - en als je doet word2ind['-'] krijg je 0 gezien die het vaakst voorkomt\n",
    "print word2ind['enron']\n",
    "print vocabulary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Extract Bag-of-Word Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_Bag_of_Word_feature(dataframe):\n",
    "    BoWs = np.zeros((len(dataframe), len(vocabulary)), dtype=np.float32)\n",
    "\n",
    "    for i, line in enumerate(dataframe):\n",
    "        for word in line.split():\n",
    "            word_ind = word2ind.get(word, -1)\n",
    "            print i\n",
    "            if(word_ind>=0):\n",
    "                BoWs[i, word_ind] += 1\n",
    "\n",
    "        if i%1000==0:\n",
    "            print(\"%4d / %d \" % (i, len(dataframe)))\n",
    "    print(\"%4d / %d \" % (i, len(dataframe)))\n",
    "            \n",
    "    return BoWs\n",
    "\n",
    "\n",
    "# Make sure use cleaned version\n",
    "train = pd.read_csv('./emails.train.csv')\n",
    "test  = pd.read_csv('./emails.test.csv')\n",
    "\n",
    "# Get labels\n",
    "Y_train = train['spam']\n",
    "Y_test  = test['spam']\n",
    "\n",
    "print(\"Extracting feature for train ...\")\n",
    "X_train = extract_Bag_of_Word_feature(train['text'])\n",
    "\n",
    "print(\"Extracting feature for test ...\")\n",
    "X_test  = extract_Bag_of_Word_feature(test[ 'text'])\n",
    "\n",
    "print('Finish.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "def eval(model, X_test, Y_test, method=''):\n",
    "    print(\"====== Performance of: {method} =======\".format(method=method))\n",
    "    \n",
    "    # Predict decision labels.\n",
    "    Y_pred  = model.predict(X_test)  \n",
    "    print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Accuracy\", \n",
    "                                              score=accuracy_score(Y_test, Y_pred)) )\n",
    "\n",
    "    # Predict confidence scores.\n",
    "    Y_score = model.decision_function(X_test)    \n",
    "    print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Average Precision\", \n",
    "                                              score=average_precision_score(Y_test, Y_score)) )\n",
    "\n",
    "    # write to submit format\n",
    "    outf = 'kaggle_data/solution.%s.csv'% method\n",
    "    with open( outf, 'w') as f:\n",
    "        f.write('id,spam\\n')\n",
    "        for i in range(len(Y_pred)):\n",
    "            # print test['id'][0]\n",
    "            f.write('%s,%s\\n' % (test['id'][i], Y_pred[i]) )\n",
    "    print(\"[output] \"+outf)\n",
    "    \n",
    "    \n",
    "# evaluate current model\n",
    "eval(model, X_test, Y_test, method='SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the positive and negative examples in test is not balance (with Nr(pos)=1707, Nr(neg)=415 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Predict all labels as negative (=0)\n",
    "print(\"====== Predict as all negative =======\")\n",
    "Y_pred_all_neg  = np.zeros((len(Y_test),), dtype=np.int)\n",
    "print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Accuracy\", \n",
    "                                          score=accuracy_score(Y_test, Y_pred_all_neg)) )\n",
    "\n",
    "# Random guess performance\n",
    "print(\"====== Predict by random guess =======\")\n",
    "Y_score_rand = np.random.uniform(0,1, (len(Y_test),))    # Generate prediction score by random\n",
    "print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Average Precision\", \n",
    "                                              score=average_precision_score(Y_test, Y_score_rand)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced: Data Preprocess\n",
    "\n",
    "However, obtaining good textural representation can be tricky as you may notice that the content of emails are noisy.\n",
    "We'll provide example code for text cleaning and you are expected to come up with smarter way to do it.\n",
    "\n",
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Om dit te runnen heb je de wordcloud module nodig.. als je op linux zit gewoon pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud(dataframe, title=None):\n",
    "    wordcloud = WordCloud(background_color=\"black\").generate(\" \".join([i for i in dataframe.str.upper()]))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "# show the word cloud of orignial dataset.\n",
    "wordcloud(df['text'], 'orignial dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### **Stop words** \n",
    " Stop Words are words which do not contain important significance to be used in Search Queries. For example, 'a', 'the', 'is', 'as', etc. Usually these words need to be filtered out because they return vast amount of unnecessary information. \n",
    "\n",
    "- ### **Stemming**\n",
    " In linguistic morphology and information retrieval, stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.\n",
    "\n",
    " Stemming programs are commonly referred to as stemming algorithms or stemmers.\n",
    "\n",
    "- ### **Lemmatization**\n",
    " Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
    "\n",
    " In computational linguistics, lemmatisation is the algorithmic process of determining the lemma for a given word. Since the process may involve complex tasks such as understanding context and determining the part of speech of a word in a sentence (requiring, for example, knowledge of the grammar of a language) it can be a hard task to implement a lemmatiser for a new language.\n",
    "\n",
    " In many languages, words appear in several inflected forms. For example, in English, the verb ‘to walk’ may appear as ‘walk’, ‘walked’, ‘walks’, ‘walking’. The base form, ‘walk’, that one might look up in a dictionary, is called the lemma for the word. The combination of the base form with the part of speech is often called the lexeme of the word.\n",
    "\n",
    " Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use python package *nltk* to do this. But before any operation, we need to download necessary nltk corpuses first with its interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download nltk corpus\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Instal Wordnet corpus                | Instal Stopwords corpus                |\n",
    "| ------------------------------------ |:--------------------------------------:|\n",
    "| ![alt text](images/nltk_wordnet.png) | ![alt text](images/nltk_stopwords.png) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_regularize(dataframe, method='lemm'):\n",
    "    print('Performing: %s ...' % method)\n",
    "    def stemming(worker, tag):\n",
    "        return worker.stem(tag)\n",
    "\n",
    "    def lemmatize(worker, tag):\n",
    "        return worker.lemmatize(tag)\n",
    "\n",
    "    if   method=='stem':\n",
    "        worker = nltk.PorterStemmer()\n",
    "        func = stemming\n",
    "    elif method=='lemm':\n",
    "        worker = nltk.WordNetLemmatizer()\n",
    "        func = lemmatize\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    for i, line in enumerate(dataframe['text']):\n",
    "#         haalt de woorden uit de dataframe\n",
    "        elems = line.strip().split()\n",
    "    \n",
    "        # apply stemming or lemmatize\n",
    "        newtags = [func(worker,tag.lower()) for tag in elems]\n",
    "        newline = \" \".join(newtags)\n",
    "\n",
    "        # update text\n",
    "        dataframe.loc[i,'text'] = newline\n",
    "\n",
    "    # return dataframe\n",
    "\n",
    "def text_filtering(dataframe, extras=set()):\n",
    "    print('Performing: filtering ...')\n",
    "    import re\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    filter_set = set(stopwords.words('english'))\n",
    "    filter_set.update(extras)\n",
    "\n",
    "    for i, line in enumerate(dataframe['text']):\n",
    "        # remove special characters with regex\n",
    "        line = re.sub(r'[^\\w]', ' ', line)\n",
    "\n",
    "        # remove digits with regex\n",
    "        line = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", line)\n",
    "\n",
    "        # remove stop words\n",
    "        elems = line.strip().split()\n",
    "        newtags = filter(lambda x: x not in filter_set, elems)\n",
    "        newline = \" \".join(newtags)\n",
    "\n",
    "        # update text\n",
    "        dataframe.loc[i, 'text'] = newline\n",
    "    # return dataframe\n",
    "    \n",
    "\n",
    "print('========= Clearn tranining data ==========')\n",
    "# Read in training data\n",
    "df = pd.read_csv('./emails.train.csv')\n",
    "\n",
    "# Do cleaning\n",
    "text_regularize(df, 'lemm')\n",
    "text_regularize(df, 'stem')\n",
    "text_filtering(df, extras=set(['subject', 'ect', 'hou', '_']))\n",
    "\n",
    "# Save as new file\n",
    "df.to_csv('emails_clean.train.csv')\n",
    "\n",
    "\n",
    "print('========= Clearn testing data ==========')\n",
    "# Read in testing data\n",
    "df = pd.read_csv('./emails.test.csv')\n",
    "\n",
    "# Do cleaning\n",
    "text_regularize(df, 'lemm')\n",
    "text_regularize(df, 'stem')\n",
    "text_filtering(df, extras=set(['subject', 'ect', 'hou', '_']))\n",
    "\n",
    "# Save as new file\n",
    "df.to_csv('emails_clean.test.csv')\n",
    "\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now show the word cloud after cleaning.\n",
    "wordcloud(df['text'], 'After text cleaning')\n",
    "\n",
    "# Based on the observation from this word cloud, you may add more non-meaningful words into `extras'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# get word frequency data for both train data\n",
    "def word_frequency_train(dataframe):\n",
    "    # for w in dataframe.str.lower():\n",
    "#     create a dataframe of all words\n",
    "    \n",
    "    datasetwords = {}\n",
    "    \n",
    "    for line in dataframe[\"text\"]:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if word in datasetwords:\n",
    "                datasetwords[word] += 1\n",
    "            else:\n",
    "                datasetwords[word] = 1\n",
    "    \n",
    "    return datasetwords\n",
    "\n",
    "# get word frequency data for mails\n",
    "def word_frequency_mail(mail):\n",
    "    words = mail.split()\n",
    "    datasetwords = {}\n",
    "    for word in words:\n",
    "        if word in datasetwords:\n",
    "            datasetwords[word] += 1\n",
    "        else:\n",
    "            datasetwords[word] = 1\n",
    "    \n",
    "    return datasetwords\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116262\n",
      "440922\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train = pd.read_csv('./emails_clean.train.csv')\n",
    "# test  = pd.read_csv('./emails_clean.test.csv')\n",
    "\n",
    "\n",
    "# # intialize initials\n",
    "# hamdata = train[train.spam == 0]\n",
    "# spamdata = train[train.spam == 1]\n",
    "# freqham = word_frequency_train(hamdata)\n",
    "# freqspam = word_frequency_train(spamdata)\n",
    "# print sum(freqspam.values())\n",
    "# print sum(freqham.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freqsword = frequency word in spam, freqhword is frequency word in ham\n",
    "def NaiveBayesian(probspam,freqsword,freqhword,totalfreqs,totalfreqh):\n",
    "    \n",
    "#   calculate probability of mail being ham\n",
    "    probham = (1-probspam)\n",
    "    \n",
    "#   calculate probability of word being in spam or word being in spam not right yet\n",
    "    probwordspam = float(freqsword)/(totalfreqs)\n",
    "    probwordham = float(freqhword)/(totalfreqh)\n",
    "    \n",
    "#  Calculate Naive bayesian\n",
    "    return (float(probwordspam*probspam)/(probwordspam*probspam + probwordham*probham))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here we call our Naive Bayesian performer with the data we read in from train and test and check\n",
    "# if the test mail is spam or not\n",
    "def CallBae(freqspam,freqham,mail,probspam,totalfreqs,totalfreqh):\n",
    "    spamchecklist = []\n",
    "     \n",
    "    for word in mail:\n",
    "    \n",
    "        if word in freqspam and word in freqham:\n",
    "#           check frequency of word in spam and ham frequency tables\n",
    "            freqsword = freqspam[word]\n",
    "            freqhword = freqham[word]\n",
    "        \n",
    "            spamchecklist.append(NaiveBayesian(probspam,freqsword,freqhword,totalfreqs,totalfreqh))\n",
    "\n",
    "    #   if the word is in neither spam or ham we put it on 0\n",
    "        elif word not in freqspam and word not in freqham:\n",
    "            continue\n",
    "\n",
    "    #  we initialize freqsword as a tiny number here cause else the Bayesian would not be able to pick up that \n",
    "    #  the word is in ham if the word never appears in spam\n",
    "        elif word in freqham and word not in freqspam:\n",
    "            \n",
    "            freqsword = 10**(-8)\n",
    "#           check freq of word in frequency table ham\n",
    "            freqhword = freqham[word]\n",
    "            spamchecklist.append(NaiveBayesian(probspam,freqsword,freqhword,totalfreqs,totalfreqh))\n",
    "\n",
    "            \n",
    "# Make spam checklist a np array and perform final test\n",
    "    spamchecklist = np.array(spamchecklist)\n",
    "    finaljudge = sum(spamchecklist/len(spamchecklist))\n",
    "    \n",
    "#   Return False if spam\n",
    "    if finaljudge > 0.5:\n",
    "        return False\n",
    "#   Return True if ham\n",
    "    if finaljudge < 0.5:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes as input frequency table of spam words, frequency table of ham words,the mail we have to check,\n",
    "# probability mail is spam, total frequency of words in spam and total frequency of words  in  ham\n",
    "def Classifier():\n",
    "    iddict = {}\n",
    "    spamdict = {}\n",
    "    idlist = []\n",
    "    spamlist = []\n",
    "\n",
    "# loop through all the mails in the test and perform naive bayesian and add id number to the list\n",
    "    for i in range(len(test)):\n",
    "        idlist.append(test['id'][i])\n",
    "        mail = test['text'][i]        \n",
    "        mail = word_frequency_mail(mail)\n",
    "              \n",
    "        if CallBae(freqspam,freqham,mail,probspam,totalfreqs,totalfreqh):\n",
    "            spamlist.append(1)\n",
    "            \n",
    "#             ####TODO#### ADD MAIL WORDS TO SPAM FREQUENCY TABLE, and total train dataframe (in order to properly\n",
    "#            calculate probspam) AND RECALCULATE TOTALFREQS AND PROBSPAM\n",
    "            \n",
    "        else:\n",
    "            spamlist.append(0)\n",
    "            \n",
    "#             ####TODO#### ADD MAIL WORDS TO HAM FREQUENCY TABLE, and total train dataframe (in order to properly\n",
    "#            calculate probspam) AND RECALCULATE TOTALFREQH AND PROBSPAM\n",
    "    \n",
    "    \n",
    "#   create our submission with per id the indicator whether the mail is spam or not (1 or 0)\n",
    "    spamdict['spam'] = spamlist\n",
    "    iddict['id'] = idlist\n",
    "    spamdict = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in spamdict.items()]))\n",
    "    iddict = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in iddict.items()]))\n",
    "    submission = pd.concat([iddict,spamdict],axis = 1)\n",
    "    submission.set_index('id',inplace=True)\n",
    "    submission.to_csv('submission.csv')\n",
    "    return submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23374340949\n"
     ]
    }
   ],
   "source": [
    "# intialize initials\n",
    "train = pd.read_csv('./emails_clean.train.csv')\n",
    "test  = pd.read_csv('./emails_clean.test.csv')\n",
    "hamdata = train[train.spam == 0]\n",
    "spamdata = train[train.spam == 1]\n",
    "freqham = word_frequency_train(hamdata)\n",
    "freqspam = word_frequency_train(spamdata)\n",
    "totalfreqs = sum(freqspam.values())\n",
    "totalfreqh = sum(freqham.values())\n",
    "spam = sum(train[\"spam\"])\n",
    "ham =  len(train[\"spam\"])-sum(train[\"spam\"])\n",
    "probspam = float(spam)/(spam+ham)\n",
    "\n",
    "\n",
    "\n",
    "# Call our  model\n",
    "submission = Classifier()\n",
    "\n",
    "# MEAN CONSEQUENTIAL EVALUATER:\n",
    "ms = 0\n",
    "testoscore = []\n",
    "for i in (test['id']):\n",
    "    try:\n",
    "        \n",
    "        if test[\"spam\"][i] != submission[\"spam\"][i]:\n",
    "            ms +=1\n",
    "        testoscore.append(test[\"spam\"][i])\n",
    "    except:\n",
    "        continue\n",
    "MCE = 1./submission.shape[0]*ms\n",
    "print MCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 2,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
